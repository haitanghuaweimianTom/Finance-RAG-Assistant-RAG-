import streamlit as st
from rag_chain import rag_qa_pipeline
from data_process import extract_clean_text_from_pdf, split_text_into_chunks
from build_vector_db import batch_insert_into_chroma, init_clients

# ===================== æ ¸å¿ƒä¿®æ”¹ï¼šé€‚é…ä½ çš„é¡¹ç›®è·¯å¾„ï¼ˆæ— é¢å¤–æ–°å¢ï¼Œå¤ç”¨å…¶ä»–æ–‡ä»¶çš„é…ç½®ï¼‰ =====================
# é¡µé¢é…ç½®
st.set_page_config(page_title="é‡‘èç ”æŠ¥é—®ç­”åŠ©æ‰‹", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆç ”æŠ¥é—®ç­”åŠ©æ‰‹")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆä¿å­˜èŠå¤©è®°å½•ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# ä¾§è¾¹æ ï¼šä¸Šä¼ PDFï¼ˆå¯é€‰ï¼Œè¡¥å……å‘é‡åº“ï¼‰
with st.sidebar:
    st.header("ğŸ“¤ è¡¥å……ç ”æŠ¥")
    uploaded_file = st.file_uploader("ä¸Šä¼ PDFç ”æŠ¥", type="pdf")
    if uploaded_file is not None:
        with st.spinner("å¤„ç†PDFå¹¶å…¥åº“..."):
            # æå–æ¸…æ´—æ–‡æœ¬
            clean_text = extract_clean_text_from_pdf(uploaded_file)
            # åˆ‡åˆ†Chunk
            chunks = split_text_into_chunks(clean_text)
            # åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶å…¥åº“
            openai_client, collection = init_clients()
            batch_insert_into_chroma(chunks, openai_client, collection)
            st.success(f"ä¸Šä¼ æˆåŠŸï¼æ–°å¢ {len(chunks)} ä¸ªç ”æŠ¥ç‰‡æ®µ")

# æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# æ¥æ”¶ç”¨æˆ·æé—®
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆå¦‚ï¼šäººå·¥æ™ºèƒ½è¡Œä¸šçš„å‘å±•å‰æ™¯å¦‚ä½•ï¼Ÿï¼‰"):
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # è°ƒç”¨RAGæµç¨‹ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ£€ç´¢ç ”æŠ¥å¹¶ç”Ÿæˆå›ç­”..."):
            response = rag_qa_pipeline(prompt)
            st.markdown(response)
    # è®°å½•åŠ©æ‰‹æ¶ˆæ¯
    st.session_state.messages.append({"role": "assistant", "content": response})